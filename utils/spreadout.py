# -*- coding: utf-8 -*-
"""SpreadOutNew.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mbo003v-LRKLTTJ9ia7Na2fSj7SnDzAO
"""
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np



def cosine_similarity(x,y):
  return torch.dot(x,y) / (torch.sqrt(torch.dot(x,x)) * torch.sqrt(torch.dot(y,y)))

def pearson_coef(x,y):
  return torch.sum((x - x.mean()) * (y-y.mean())) / (torch.sqrt(torch.sum(torch.pow(x-x.mean(),2))) * torch.sqrt(torch.sum(torch.pow(y-y.mean(),2))))

def euclidean_distance(x,y):
  return torch.sqrt(torch.sum(torch.pow((x-y),2)+1e-07))

def RVcoef(dataList):
    # First compute the scalar product matrices for each data set X
    scalArrList = []

    for arr in dataList:
        arr = arr.cuda()
        scalArr = torch.mm(arr, torch.t(arr))
        scalArrList.append(scalArr.cuda())

    # Now compute the 'between study cosine matrix' C
    C = torch.zeros((len(dataList), len(dataList)))

    for index, element in np.ndenumerate(C):
        nom = torch.trace(torch.mm(torch.t(scalArrList[index[0]]),
                                    scalArrList[index[1]]))
        denom1 = torch.trace(torch.mm(torch.t(scalArrList[index[0]]),
                                       scalArrList[index[0]]))
        denom2 = torch.trace(torch.mm(torch.t(scalArrList[index[1]]),
                                       scalArrList[index[1]]))

        Rv = nom
        d = torch.mm(denom1.reshape(1,1), denom2.reshape(1,1))
        d = torch.sqrt(d)
        Rv = nom / d

        #Rv = nom / torch.sqrt(torch.mm(denom1, denom2))
        C[index[0], index[1]] = Rv.cuda()

    return C.cuda()


def check_pearson_sum(layer):
  with torch.no_grad():
    nchannel = layer.weight.shape[1]
    nfilters = layer.weight.shape[0]
    kernel_size = layer.weight.shape[2]
    loss = 0
    for i in range(nfilters): # iterate over filters 0-> 64
      for j in range(nfilters): # iterate over filters 0-> 64

        res = pearson_coef(layer.weight[i].reshape(nchannel * kernel_size**2),
                           layer.weight[j].reshape(nchannel * kernel_size**2))

        if res.item() > 0:
          loss += res
        else:
          loss +=  - res

    print(loss)
  return loss

def layer_pearson_sum(layer,optimizer):
  """
  "" Backwards and returns the sum of the pearson correlation between all pairs of filters inside a given kernel
  """
  nchannel = layer.weight.shape[1]
  nfilters = layer.weight.shape[0]
  kernel_size = layer.weight.shape[2]
  #model.train()

  optimizer.zero_grad()
  loss = 0
  for i in range(nfilters): # iterate over filters 0-> 64
    for j in range(nfilters): # iterate over filters 0-> 64

      res = pearson_coef(layer.weight[i].reshape(nchannel * kernel_size**2),
                         layer.weight[j].reshape(nchannel * kernel_size**2))

      if res.item() > 0:
        loss += res
      else:
        loss +=  - res

  print(loss)
  loss.backward()
  optimizer.step()

  return loss.item()


def check_rv_sum(layer):
  with torch.no_grad():
    nfilters = layer.weight.shape[0]
    nchannel = layer.weight.shape[1]
    filter_size = layer.weight.shape[2]

    rv_sum = 0
    dataList = []
    for i in range(nfilters): # iterate over filters 0-> 64
      X = layer.weight[i].reshape(nchannel , filter_size**2)  ## shapes of (3,121) ??
      dataList.append(X)

    rv_results = RVcoef(dataList)
    rv_sum += rv_results.triu().sum() - rv_results.diag().sum()
    #print(rv_sum)
    return rv_sum.item()

def layer_rv_sum(layer,optimizer):
  """
  "" Backwards and returns the sum of the RV coefficients channel by channel between all pairs of filters inside
  "" a given nn.Conv2d
  """
  nfilters = layer.weight.shape[0]
  nchannel = layer.weight.shape[1]
  filter_size = layer.weight.shape[2]


  rv_sum = 0
  dataList = []
  optimizer.zero_grad()
  for i in range(nfilters): # iterate over filters 0-> 64
    X = layer.weight[i].reshape(nchannel , filter_size**2)  ## shapes of (3,121) ??
    dataList.append(X)

  rv_results = RVcoef(dataList)
  rv_sum += rv_results.triu().sum() - rv_results.diag().sum()

  #print(rv_sum)
  rv_sum.backward()
  optimizer.step()

  return rv_sum.item()


def check_euclidean_sum(layer):
  """
  "" Returns the sum of the euclidean distance between all pairs of filters in a nn.Conv2d
  """
  with torch.no_grad():
    nfilters = layer.weight.shape[0]
    nchannel = layer.weight.shape[1]
    filter_size = layer.weight.shape[2]
    loss = 0
    for p in layer.weight: # iterate over filters 0-> 64
      for q in layer.weight: # iterate over filters 0-> 64
        loss += euclidean_distance(p,q)

    print(loss)
  return loss.item()

def layer_euclidean_sum(layer, optimizer):
  """
  "" backwards the euclidean distance sum
  """
  nfilters = layer.weight.shape[0]
  nchannel = layer.weight.shape[1]
  filter_size = layer.weight.shape[2]

  optimizer.zero_grad()
  loss = 0
  for p in layer.weight: # iterate over filters 0-> 64
    for q in layer.weight: # iterate over filters 0-> 64
      loss += euclidean_distance(p,q)

  print(loss)
  loss = -loss
  loss.backward()
  optimizer.step()

  return -loss.item()



class Spreadout:
  def __call__(self,model):
    if self.mode == 'euclidean':
      self.auto_spread_distance(model)
    else:
      self.auto_spread(model)

  def __init__(self, gamma, mode):
    print("CREATING INITIALIZER ", mode)
    self.gamma = gamma
    self.mode = mode

    if mode == 'euclidean':
      print("FOUND MODE EUCLID")
      self.spread_fn = layer_euclidean_sum
      self.check_fn = check_euclidean_sum
    elif mode == 'rv':
      print("FOUND MODE RV")
      self.spread_fn = layer_rv_sum
      self.check_fn = check_rv_sum
    else:
      print("FOUND MODE PEARSON")
      self.spread_fn = layer_pearson_sum
      self.check_fn = check_pearson_sum


  def spread(self, model, epochs, gamma, layer_slice=None):

    model.train()
    layers = []

    for layer in model.modules():
      if isinstance(layer, nn.Conv2d):
        layers.append(layer)

    if layer_slice is not None:
        # Check if is slice else is a list
        if isinstance(layer_slice,slice):
            tmp = layers[layer_slice]
        else:
            tmp = [layers[l] for l in layer_slice]
        layers = tmp
        print("Applying spreadout on layers", layer_slice)


    print("Using fn", self.spread_fn)
    print("Using check fn", self.check_fn)
    print("Using gamma", gamma)
    print(layers)
    
    for epoch in range(epochs):
      for i,layer in enumerate(layers):
        print("Applying", gamma, "to layer", i)
        optimizer = optim.SGD(model.parameters(), lr=gamma)
        self.spread_fn(layer, optimizer)


  def auto_spread(self,model):
    model.train()
    layers = []

    for layer in model.modules():
      if isinstance(layer, nn.Conv2d):
        layers.append(layer)

    model.train()
    for layer in layers: # spread
      gamma = 1.0
      best_sum = self.check_fn(layer)
      torch.save(model.state_dict(), "best_spread_pearson")

      while True:
        optimizer = optim.SGD(model.parameters(), lr=gamma)
        ksum = 0
        l = self.spread_fn(layer, optimizer)
        print("Sum before backwards: ", l)
        ksum = self.check_fn(layer)
        print("Sum after backwards: ", ksum)
        print("Best sum:", best_sum)
        #break
        if ksum < best_sum and best_sum - ksum > 50:
          best_sum = ksum
          torch.save(model.state_dict(), "best_spread_" + str(self.mode))
        else:
          gamma *= 0.1
          model.load_state_dict(torch.load("best_spread_" + str(self.mode)))
          if gamma < 0.00001 or ksum == 0:
            break
          print("Spread went wrong!")
        print(".........")



  def auto_spread_distance(self, model):
    model.train()
    layers = []

    for layer in model.modules():
      if isinstance(layer, nn.Conv2d):
        layers.append(layer)

    model.train()
    for layer in layers: # spread
      gamma = 1.0
      best_sum = self.check_fn(layer)
      torch.save(model.state_dict(), "best_spread_distance")

      while True:
        optimizer = optim.SGD(model.parameters(), lr=gamma)
        ksum = 0
        l = self.spread_fn(layer, optimizer)
        print("Sum before backwards: ", l)
        ksum = self.check_fn(layer)
        print("Sum after backwards: ", ksum)
        print("Best sum:", best_sum)
        #break
        if ksum > best_sum:
          best_sum = ksum
          torch.save(model.state_dict(), "best_spread_distance")
        else:
          gamma *= 0.1
          model.load_state_dict(torch.load("best_spread_distance"))
          if gamma < 0.0000001 or ksum == 0:
            break
          print("Spread went wrong! new gamma = ", gamma)
        print(".........")
